# Spec 02: wechat-channel - Session Replay

## Session: 2026-02-19T20:46 ~ 2026-02-20T23:02

> Person: kweng | Messages: 424 | Turns: 11

---

### 20:46 - Implement the following plan:

**Prompt:**

> Implement the following plan:
>
> # Plan: Three-Tier Session History + TASK Entity Type
>
> ## Context

<details>
<summary>Full prompt (6223 chars)</summary>

Implement the following plan:

# Plan: Three-Tier Session History + TASK Entity Type

## Context

The session history system currently puts `history/` directories inside EVERY entity directory — including source code (`src/whatsapp/history/`), docs (`docs/channels/history/`), and extensions (`extensions/wechat/history/`). This pollutes 19 code/content directories in openclaw and 8 in Enpack_CCC with generated files.

Only SPEC entities (and the new TASK type) should have inline history. All other entity types should store history centrally under `会话历史/entities/`.

Additionally, there's no TASK entity type for operational work that doesn't warrant a full spec.

Changes go to `~/AI/base` first (canonical source), then pull into both projects.

---

## Step 1: Modify `~/AI/base/src/session_history/models/category.py`

**Add TASK to EntityType, replace history_dir property with field:**

```python
class EntityType(Enum):
    SPEC = "spec"
    TASK = "task"              # NEW
    SOURCE = "source"
    RESEARCH = "research"
    KNOWLEDGE = "knowledge"
    TOOL = "tool"
    UNCATEGORIZED = "uncategorized"

INLINE_HISTORY_TYPES = {EntityType.SPEC, EntityType.TASK}
```

Replace the `history_dir` `@property` with a plain field `history_dir: str = ""` — set at construction time by EntityRegistry rather than computed.

---

## Step 2: Modify `~/AI/base/src/session_history/config/entity_registry.py`

**Three changes:**

### 2a: Add TASK discovery

```python
_TASK_DIRS = ["tasks", "任务"]
```

New `_discover_tasks()` method — same pattern as `_discover_specs()` but also accepts plain kebab-case names (not just `##_name`).

### 2b: Add history dir resolution

New `_resolve_history_dirs()` method called at end of `discover_all()`:

```python
def _resolve_history_dirs(self, entities):
    for entity in entities:
        if entity.entity_type in INLINE_HISTORY_TYPES:
            entity.history_dir = f"{entity.directory}/history"
        else:
            entity.history_dir = f"{self.history_root}/entities/{entity.entity_type.value}/{entity.name}"
```

### 2c: Update constructor and discover_all()

Constructor takes `history_root: str = "会话历史"` parameter. `discover_all()` calls `_discover_tasks()` and `_resolve_history_dirs()`.

---

## Step 3: Modify `~/AI/base/src/session_history/main.py`

Pass `settings.history_root` when constructing `EntityRegistry` (2 call sites: `cmd_scan` line 49, `_load_entity_index` line 174).

---

## Step 4: Modify `~/AI/base/src/session_history/generator/readable_replay_generator.py`

Pass `history_root` to `EntityRegistry` constructor (1 call site).

---

## Step 5: Modify `~/AI/base/src/session_history/config/settings.py`

Add `"task": "任务"` to the `entity_dirs` dict (backward compat).

---

## Step 6: Update tests in `~/AI/base`

- All test Entity objects need explicit `history_dir=` field (no longer auto-computed)
- Files: `tests/test_generators.py`, `tests/test_classifier.py`
- Run: `cd ~/AI/base/src && python3 -m pytest session_history/tests/ -v`

---

## Step 7: Commit and push base

```bash
cd ~/AI/base
git add src/session_history/
git commit -m "Add TASK entity type; three-tier history storage (inline/centralized/uncategorized)"
git push origin main
```

---

## Step 8: Pull into openclaw, clean up, re-scan

```bash
cd ~/AI/openclaw
git fetch base && git merge base/main
```

Delete old history dirs from source/docs/extensions:

```bash
rm -rf src/{types,config,plugins,web,terminal,shared,cli,line,whatsapp,channels}/history
rm -rf docs/{tools,install,plugins,cli,channels}/history
rm -rf extensions/{open-prose,feishu,wechat,google-gemini-cli-auth}/history
```

Re-scan:

```bash
cd src && python3 -m session_history scan
```

---

## Step 9: Pull into Enpack_CCC, clean up, re-scan

Add base remote (Enpack_CCC doesn't have one yet):

```bash
cd ~/AI/Enpack_CCC
git remote add base https://github.com/kevinw99/ai-project-base.git
git fetch base && git merge base/main --allow-unrelated-histories
```

Resolve conflicts: accept base version at `src/session_history/`, delete old `源代码/session_history/`.

Update the session_history hook in `.claude/settings.json` to use new path:

```
cd "$(git rev-parse --show-toplevel)/src" && python3 -m session_history scan -i
```

Delete old history dirs:

```bash
rm -rf 源代码/{chunked_processor,experiment_analyzer,doc_indexer,session-persistence}/history
rm -rf 知识库/{01_公司档案,99_图片资料归档}/history
rm -rf 工具/history 研究/状态报告/history
rm -rf 源代码/session_history  # old standalone copy
```

Re-scan:

```bash
cd src && python3 -m session_history scan
```

---

## Step 10: Verify both repos

For each repo:

1. `python3 -m session_history stats` — same session counts as before
2. `python3 -m session_history list` — all sessions classified
3. No `history/` dirs remain under src/, extensions/, docs/, 源代码/, 知识库/, 工具/
4. Spec history dirs still work inline: `spec/*/history/`
5. Centralized history exists: `会话历史/entities/source/*/sessions-index.json`
6. Replay works for both inline and centralized entities

---

## Files Changed (all in ~/AI/base/src/session_history/)

| File                                     | Change                                                                            |
| ---------------------------------------- | --------------------------------------------------------------------------------- |
| `models/category.py`                     | Add TASK enum, INLINE_HISTORY_TYPES, replace history_dir property → field         |
| `config/entity_registry.py`              | Add \_TASK_DIRS, \_discover_tasks(), \_resolve_history_dirs(), update constructor |
| `config/settings.py`                     | Add "task" to entity_dirs                                                         |
| `main.py`                                | Pass history_root to EntityRegistry (2 sites)                                     |
| `generator/readable_replay_generator.py` | Pass history_root to EntityRegistry (1 site)                                      |
| `tests/test_generators.py`               | Set history_dir field on Entity objects                                           |
| `tests/test_classifier.py`               | Set history_dir field on Entity objects                                           |

## After Pulling (per project)

| Action                          | openclaw       | Enpack_CCC                |
| ------------------------------- | -------------- | ------------------------- |
| Delete old history dirs         | 19 dirs        | 8 dirs                    |
| Delete old session_history copy | N/A            | `源代码/session_history/` |
| Add base remote                 | Already exists | New                       |
| Re-scan                         | Yes            | Yes                       |

If you need specific details from before exiting plan mode (like exact code snippets, error messages, or content you generated), read the full transcript at: /Users/kweng/.claude/projects/-Users-kweng-AI-openclaw/9347e2ce-4961-4b2d-8722-cdfcbda16ccf.jsonl

</details>

**Result:**
All done. Here's a summary of what was implemented:

## Summary

### Base changes (`~/AI/base`) — committed and pushed

- **`models/category.py`**: Added `TASK` enum value, `INLINE_HISTORY_TYPES` set, replaced `history_dir` property with plain field
- **`config/entity_registry.py`**: Added `_TASK_DIRS`, `_discover_tasks()` (supports numbered + kebab-case names), `_resolve_history_dirs()` (inline for SPEC/TASK, centralized for others), `history_root` constructor param
- **`main.py`** + **`readable_replay_generator.py`**: Pass `history_root` to `EntityRegistry` (3 call sites)
- **`config/settings.py`**: Added `"task": "任务"` to `entity_dirs`
- **Tests**: All Entity objects updated with explicit `history_dir`; 11/11 pass

### Openclaw — merged and cleaned

- Merged base/main, resolved conflicts (accepted base for session_history, kept openclaw for skill files)
- Deleted 23 old inline `history/` dirs from `src/`, `docs/`, `extensions/`
- Re-scanned: 10 sessions, 6 classified, 4 uncategorized
- Centralized history: `会话历史/entities/source/` (14 entities), `会话历史/entities/knowledge/` (5 entities)
- Spec history remains inline: `spec/01_*/history/`, `spec/02_*/history/`

### Enpack_CCC — merged and cleaned

- Added `base` remote, merged with `--allow-unrelated-histories`
- Deleted old `源代码/session_history/` (replaced by `src/session_history/` from base)
- Deleted 8 old inline `history/` dirs
- Updated SessionEnd hook path: `源代码` → `src`
- Re-scanned successfully (0 sessions — expected, no JSONL files)

_Tools: Bash (57), Edit (20), Read (13), TaskCreate (9), TaskUpdate (18) -- .claude/settings.json, /Users/kweng/AI/base/src/session_history/config/entity_registry.py, /Users/kweng/AI/base/src/session_history/config/settings.py, /Users/kweng/AI/base/src/session_history/generator/index_generator.py, /Users/kweng/AI/base/src/session_history/generator/readable_replay_generator.py +5 more -- Run session_history tests; Check python setup and install pytest if needed; Install pytest_

---

### 21:01 - so how do I make it a task, so session is tied to that...

**Prompt:**

> so how do I make it a task, so session is tied to that entiry?

**Result:**
Create a directory under `tasks/` (or `任务/`) in your project root. The entity registry auto-discovers from there, just like specs.

**Two naming conventions work:**

1. **Numbered** (like specs): `tasks/01_migrate-history-storage/`
2. **Plain kebab-case**: `tasks/migrate-history-storage/`

That's it. Next time `session_history scan` runs, it'll discover the task entity and classify sessions that reference files/keywords in that directory.

**Example for this session's work:**

```bash
mkdir -p tasks/01_three-tier-history
```

Then put a brief description file in it (optional, but helps classification since the classifier matches on path patterns and keywords):

```
tasks/01_three-tier-history/README.md
```

The history for that task will be stored inline at `tasks/01_three-tier-history/history/` (same as specs, since TASK is in `INLINE_HISTORY_TYPES`).

**To retroactively classify this session**, just re-scan after creating the directory:

```bash
cd src && python3 -m session_history scan
```

The classifier will match based on path patterns (`tasks/01_three-tier-history/`) and keywords extracted from the directory name (`three`, `tier`, `history`, etc.).

---

### 21:08 - " 1. Numbered (like specs): tasks/01_migrate-history-storag...

**Prompt:**

> " 1. Numbered (like specs): tasks/01_migrate-history-storage/  
> " can you make this a part of wrokflow, so when user says "careate a task, a numbered one will be created.

**Result:**
Both merged cleanly. Now you can use `/create-task` in any project. The workflow:

1. **Say** `/create-task three-tier history storage` (or just `/create-task` and I'll ask)
2. **It creates** `tasks/01_three-tier-history-storage/README.md` with the next available number
3. **Sessions** that touch files in that directory get auto-classified to the task entity
4. **History** lives inline at `tasks/01_three-tier-history-storage/history/` (same as specs)

_Tools: Bash (5), Read (4), Task (1), Write (1) -- /Users/kweng/AI/base/.claude/commands/log.md, /Users/kweng/AI/base/.claude/skills/create-task.md, /Users/kweng/AI/base/.claude/skills/history.md, /Users/kweng/AI/base/specs/00_template, /Users/kweng/AI/base/specs/01_task-management/README.md -- Check for existing tasks/specs dirs in base; List template contents; Commit and push create-task skill_

---

### 23:19 - 关于微信channel的实现办法你也提到了这个

**Prompt:**

> 关于微信channel的实现办法你也提到了这个
> " Approach: wxauto (GUI automation)
> Status: Working, slow
> Risk: Moderate"
> 你提到了这个工具，那么用图形界面自动化就像人工手工的一页一页去翻是吧？我想再extend

<details>
<summary>Full prompt (611 chars)</summary>

关于微信channel的实现办法你也提到了这个
" Approach: wxauto (GUI automation)
Status: Working, slow
Risk: Moderate"
你提到了这个工具，那么用图形界面自动化就像人工手工的一页一页去翻是吧？我想再extend
一下。openclaw有没有这方面的功能？就说如果有这方面的功能的话，能不能用openclaw来实现这个功能？如果没有的话，我们是自己开发一个这样的功能，还是说我们找一个第三方的工具来做这个事情？如果我们自己开发的话，我们需要考虑哪些技术难点？如果我们找第三方工具的话，我们需要考虑哪些因素来选择合适的工具？请你分析一下这两种方案的优缺点，并给出你的建议。
另外，我要把它做成一个通用的功能，就是用图形自动化，我可以达到其他的目的:
比如说其他AI工具像豆包ChatGPT Grok, 我都有很多的交谈的信息里面就有我很多的个人材料. 有没有API能够直接读到以前的交谈历史？如果没有的话，我们也可以用图形界面自动化的方式来把这些交谈历史读出来，构建成一个个人的知识库，这样的话就可以在不同的AI工具里面都能够调用到这个个人的知识库了。你觉得这个想法怎么样？如果可行的话，我们应该怎么去实现它呢？需要考虑哪些技术难点？需要考虑哪些隐私和安全方面的问题？请你分析一下这个想法的可行性，并给出你的建议。

</details>

**Result:**
这是个很有远见的想法。我来系统分析一下。

---

## 1. wxauto 和图形界面自动化

对，wxauto 本质就是模拟人工操作——找到微信窗口、模拟鼠标滚动、用 Win32 API 读取消息控件内容。速度慢是因为它真的在"翻页"。

## 2. openclaw 有现成的浏览器自动化能力

openclaw 已经内置了 **Playwright Core** 驱动的完整浏览器自动化：

| 能力                        | 说明                                         |
| --------------------------- | -------------------------------------------- |
| `snapshot`                  | 获取页面可访问性快照（结构化 DOM）           |
| `screenshot`                | 截屏 + AI 视觉理解                           |
| `click` / `type` / `scroll` | 模拟交互                                     |
| `evaluate`                  | 在页面内执行 JS                              |
| `navigate` / `open tab`     | 页面导航                                     |
| Chrome 扩展                 | 控制已有的 Chrome 标签页（不需要新开浏览器） |

**这意味着：对于 Web 端的 AI 工具（ChatGPT、Grok、豆包 web 版），openclaw 已经可以做 GUI 自动化提取。** 不需要从零开发。

但 wxauto 针对的是 **微信桌面客户端**（Win32 原生应用），Playwright 控制不了它——Playwright 只能控制浏览器。

## 3. 各平台 API 和提取方案对比

| 平台        | 官方导出/API                                                 | GUI 自动化可行性                  |
| ----------- | ------------------------------------------------------------ | --------------------------------- |
| **ChatGPT** | 有导出功能（Settings → Export data），返回 JSON 包含所有对话 | Web 端也可用 Playwright           |
| **Claude**  | JSONL 已在本地 `~/.claude/projects/`                         | 已有 session_history 处理         |
| **Grok**    | 无官方导出 API                                               | Web 端可用 Playwright             |
| **豆包**    | 无官方导出 API                                               | Web 端可用 Playwright，但反爬严格 |
| **微信**    | 无 API，PC 端加密存储                                        | wxauto（Win32）或解密本地数据库   |

**建议优先级**：先用官方 API/导出 → 再考虑 GUI 自动化。

## 4. 方案分析：自己开发 vs 第三方

### 方案 A：基于 openclaw 浏览器自动化（推荐）

**优点**：

- Playwright 已内置，不需要额外依赖
- 可以写成 openclaw skill/plugin，对话式操作："/extract-history chatgpt"
- AI 视觉理解可以处理动态加载、验证码等异常情况
- 跨平台（macOS/Linux），不像 wxauto 只能 Windows

**缺点**：

- Web 版 UI 经常改版，选择器会失效（需要维护）
- 有些平台有反自动化检测（Cloudflare、验证码）
- 速度仍然受限于页面加载

**技术难点**：

- 无限滚动加载（需要滚动到底+等待+去重）
- 登录态管理（cookie/session 保持）
- 数据结构解析（不同平台 DOM 结构不同）
- 并发和速率控制（避免被封号）

### 方案 B：第三方工具

| 工具                             | 特点                          |
| -------------------------------- | ----------------------------- |
| **DoubleCommander / WebScraper** | 通用爬虫，不够智能            |
| **n8n / Zapier**                 | 工作流工具，依赖平台有 API    |
| **ArchiveBox**                   | 网页归档工具，不解析对话结构  |
| **chatgpt-export** (开源)        | 只针对 ChatGPT，解析导出 JSON |

**结论**：没有一个第三方工具能通用地从多个 AI 平台提取对话历史。各个都是针对单一平台的。

### 推荐：方案 A + 分层架构

```
openclaw skill: /extract-history <platform>
    ├─ 数据获取层（per-platform adapter）
    │   ├─ chatgpt: 优先用官方 Export，fallback Playwright
    │   ├─ grok: Playwright 提取
    │   ├─ doubao: Playwright 提取
    │   └─ wechat: wxauto (Windows) 或第三方解密工具
    ├─ 数据标准化层
    │   └─ 统一为: {platform, timestamp, role, content} JSONL
    └─ 知识库集成层
        └─ 导入到 openclaw memory / 本地向量数据库
```

## 5. 隐私和安全考虑

- **本地处理**：所有数据应该留在本地，不上传第三方服务
- **加密存储**：提取的对话可能包含敏感信息，存储时应加密
- **权限隔离**：不同平台的 cookie/session 不应混在一起
- **数据分级**：个人闲聊 vs 工作内容 vs 敏感信息，应该可以标记和过滤
- **合规性**：微信数据提取可能触及腾讯服务条款，需要注意

## 6. 如果要做，下一步建议

1. **先做 ChatGPT**——它有官方导出，最简单，验证整个流程
2. **再做 Grok/豆包 web 版**——用 openclaw Playwright，积累通用提取逻辑
3. **微信最后做**——最难，需要 Windows 环境或数据库解密方案
4. 整个做成一个 **openclaw plugin**，这样在任何 openclaw 项目里都能用

要不要先建一个 task 来跟踪这件事？比如 `/create-task personal-knowledge-extraction`

_Tools: Task (1)_

---

### 23:31 - yes

**Prompt:**

> yes

**Result:**
Created `tasks/01_personal-knowledge-extraction/`. Now future sessions that reference this task will be auto-classified to it, and history will live inline at `tasks/01_personal-knowledge-extraction/history/`.

Want to start on Phase 1 (ChatGPT export parser) now, or save it for a dedicated session?

_Tools: Bash (2), Skill (1), Write (1) -- /Users/kweng/AI/openclaw/tasks/01_personal-knowledge-extraction/README.md -- Find highest task number; Create task directory_

---

### 23:53 - why task, no spec here

**Prompt:**

> why task, no spec here

**Result:**
You're right — this is a spec, not a task. The distinction:

- **Task**: Operational one-offs — migrations, cleanups, investigations. Short-lived, no design needed.
- **Spec**: Feature development with requirements, design, multiple phases. Lives longer, needs architecture.

"Personal Knowledge Extraction" is clearly a feature with multi-phase development, platform adapters, a plugin architecture. That's a spec.

Want me to move it to `spec/03_personal-knowledge-extraction/` and flesh it out with the full spec template (requirements, design, tasks, status)?

---

### 23:56 - yes

**Prompt:**

> yes

**Result:**
You've hit your limit · resets 8am (Asia/Shanghai)

---

### 07:31 - Session History

**Prompt:**

> # Session History
>
> Manage and explore Claude Code session history using the session_history tool.
>
> ## How to Run

<details>
<summary>Full prompt (1116 chars)</summary>

# Session History

Manage and explore Claude Code session history using the session_history tool.

## How to Run

The tool is at `src/session_history/`. First determine the project root via `git rev-parse --show-toplevel`, then run:

```bash
cd "$(git rev-parse --show-toplevel)/src" && python3 -m session_history <command> [options]
```

## Available Commands

Based on user input (or default to `list` if no specific command mentioned):

- **scan** — Scan all sessions and classify them. Use `--incremental` / `-i` for incremental scan.
- **list** — List all sessions with classifications. Use `--type <type>` to filter.
- **replay `<entity>`** — Generate readable replay for an entity. Use `--raw` for old-format HTML/Markdown.
- **search `<query>`** — Search session content. Use `--limit N` to control matches per session.
- **stats** — Show classification statistics.

## Instructions

1. Parse the user's argument (e.g., `/history scan`, `/history search auth`, `/history stats`)
2. If no argument is given, run `list` as the default
3. Run the appropriate command using Bash
4. Present the output to the user

</details>

**Result:**
Incremental scan picked up 2 new/modified sessions:

- **`36454a45`** (377 msgs) → **Spec 03: personal-knowledge-extraction** (0.20) + Source: knowledge_harvester (0.17)
- **`b86636af`** (409 msgs) → Spec 02 (0.11) + **Spec 03** (0.11)

Spec 03 is now live with 2 sessions classified to it. The `knowledge_harvester` source entity was also auto-discovered from `src/knowledge_harvester/`.

_Tools: Bash (1) -- Incremental scan of session history_

---

### 23:00 - history

**Prompt:**

> history

**Result:**
_(no text response — tools only)_

---

### 23:00 - Session History

**Prompt:**

> # Session History
>
> Manage and explore Claude Code session history using the session_history tool.
>
> ## How to Run

<details>
<summary>Full prompt (1116 chars)</summary>

# Session History

Manage and explore Claude Code session history using the session_history tool.

## How to Run

The tool is at `src/session_history/`. First determine the project root via `git rev-parse --show-toplevel`, then run:

```bash
cd "$(git rev-parse --show-toplevel)/src" && python3 -m session_history <command> [options]
```

## Available Commands

Based on user input (or default to `list` if no specific command mentioned):

- **scan** — Scan all sessions and classify them. Use `--incremental` / `-i` for incremental scan.
- **list** — List all sessions with classifications. Use `--type <type>` to filter.
- **replay `<entity>`** — Generate readable replay for an entity. Use `--raw` for old-format HTML/Markdown.
- **search `<query>`** — Search session content. Use `--limit N` to control matches per session.
- **stats** — Show classification statistics.

## Instructions

1. Parse the user's argument (e.g., `/history scan`, `/history search auth`, `/history stats`)
2. If no argument is given, run `list` as the default
3. Run the appropriate command using Bash
4. Present the output to the user

</details>

**Result:**
2 sessions, both classified:

| Session    | Date       | Msgs | Primary Entity                                    | Secondary                                     |
| ---------- | ---------- | ---- | ------------------------------------------------- | --------------------------------------------- |
| `36454a45` | 2026-02-19 | 377  | **Spec 03: personal-knowledge-extraction** (0.20) | Source: knowledge_harvester (0.17)            |
| `b86636af` | 2026-02-19 | 409  | **Spec 02: wechat-channel** (0.11)                | Spec 03: personal-knowledge-extraction (0.11) |

Note: the previous full scan's sessions got cleared by the incremental scan (it only re-indexed the 2 modified files). If you want a complete picture, run `/history scan` (full, non-incremental) to re-classify all 11 sessions.

_Tools: Bash (1) -- List all sessions with classifications_

---

### 23:01 - history

**Prompt:**

> history

**Result:**
_(no text response — tools only)_

---
