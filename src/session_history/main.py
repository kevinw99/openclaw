"""CLI å…¥å£ - ä¼šè¯å†å²åˆ†ç±»ç³»ç»Ÿ"""

import argparse
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# ç¡®ä¿æ¨¡å—å¯ä»¥è¢«æ­£ç¡®å¯¼å…¥
sys.path.insert(0, str(Path(__file__).parent.parent))

from session_history.config.settings import Settings
from session_history.config.entity_registry import EntityRegistry
from session_history.parser.jsonl_reader import JsonlReader
from session_history.classifier.composite_classifier import CompositeClassifier
from session_history.generator.index_generator import IndexGenerator
from session_history.generator.html_generator import HtmlGenerator
from session_history.generator.markdown_generator import MarkdownGenerator
from session_history.generator.readable_replay_generator import ReadableReplayGenerator
from session_history.generator.replay_index_generator import ReplayIndexGenerator
from session_history.models.category import Entity, EntityMatch, EntityType, SessionClassification
from session_history.models.index import SessionReference


def cmd_scan(args):
    """æ‰«ææ‰€æœ‰ä¼šè¯å¹¶åˆ†ç±»"""
    settings = Settings()
    if args.sessions_dir:
        settings.sessions_dir = Path(args.sessions_dir)

    print("=" * 60)
    print("ä¼šè¯å†å²åˆ†ç±»ç³»ç»Ÿ - æ‰«æ")
    print("=" * 60)
    print(f"ä¼šè¯ç›®å½•: {settings.sessions_dir}")
    print(f"é¡¹ç›®æ ¹: {settings.project_root}")

    # å¢é‡æ¨¡å¼æ£€æŸ¥
    last_scan = {}
    if args.incremental and settings.scan_state_path.exists():
        with open(settings.scan_state_path, "r", encoding="utf-8") as f:
            last_scan = json.load(f)
        print("æ¨¡å¼: å¢é‡æ‰«æ")
    else:
        print("æ¨¡å¼: å…¨é‡æ‰«æ")

    print("-" * 60)

    # 1. å‘ç°å®ä½“
    registry = EntityRegistry(settings.project_root, history_root=settings.history_root)
    entities = registry.discover_all()
    print(f"\n[1/4] å‘ç° {len(entities)} ä¸ªå®ä½“")
    for e in entities:
        print(f"  - {e.display_name} ({e.entity_type.value})")

    # 2. è¯»å–ä¼šè¯
    reader = JsonlReader(
        exclude_thinking=settings.exclude_thinking,
        exclude_sidechains=settings.exclude_sidechains,
    )
    session_files = reader.list_session_files(str(settings.sessions_dir))

    # å¢é‡è¿‡æ»¤
    if args.incremental and last_scan:
        filtered = []
        for f in session_files:
            mtime = os.path.getmtime(f)
            last_mtime = last_scan.get("file_mtimes", {}).get(f, 0)
            if mtime > last_mtime:
                filtered.append(f)
        if not filtered:
            print(f"\n[2/4] æ— æ–°å¢æˆ–ä¿®æ”¹çš„ä¼šè¯æ–‡ä»¶ï¼Œè·³è¿‡æ‰«æ")
            return
        print(f"\n[2/4] å¢é‡æ‰«æ {len(filtered)}/{len(session_files)} ä¸ªä¼šè¯æ–‡ä»¶")
        session_files = filtered
    else:
        print(f"\n[2/4] è¯»å– {len(session_files)} ä¸ªä¼šè¯æ–‡ä»¶")

    sessions = []
    for fp in session_files:
        try:
            session = reader.read_session(fp)
            sessions.append(session)
            print(f"  âœ“ {Path(fp).stem[:8]}... ({session.message_count} msgs)")
        except Exception as e:
            print(f"  âœ— {Path(fp).stem[:8]}... é”™è¯¯: {e}")

    # 3. åˆ†ç±»
    print(f"\n[3/4] åˆ†ç±» {len(sessions)} ä¸ªä¼šè¯...")
    classifier = CompositeClassifier(settings)
    new_classifications = []
    for session in sessions:
        classification = classifier.classify(session, entities)
        new_classifications.append(classification)
        if classification.matches:
            top = classification.matches[0]
            print(f"  {session.session_id[:8]}... -> {top.entity.display_name} ({top.confidence:.2f})")
            for m in classification.matches[1:3]:
                print(f"    + {m.entity.display_name} ({m.confidence:.2f})")
        else:
            print(f"  {session.session_id[:8]}... -> Uncategorized")

    # å¢é‡æ¨¡å¼: åˆå¹¶æ–°åˆ†ç±»åˆ°å·²æœ‰åˆ†ç±»
    history_dir = settings.history_dir
    if args.incremental:
        classifications = _merge_classifications(new_classifications, history_dir)
    else:
        classifications = new_classifications

    # 4. ç”Ÿæˆç´¢å¼•
    print(f"\n[4/4] ç”Ÿæˆç´¢å¼•...")
    index_gen = IndexGenerator(settings.project_root)

    # æŒ‰å®ä½“ç”Ÿæˆç´¢å¼• (session å½’å…¥æ‰€æœ‰åŒ¹é…çš„å®ä½“, æ”¯æŒå¤šå®ä½“ session æ‹†åˆ†)
    entity_refs = {}  # entity_id -> [SessionReference]
    # ä¸ºåˆå¹¶åçš„å…¨é‡åˆ†ç±»æ„å»ºå¼•ç”¨, éœ€è¦åŠ è½½æ‰€æœ‰ session å¯¹è±¡
    all_sessions_map = {s.session_id: s for s in sessions}  # å·²åŠ è½½çš„ session
    for classification in classifications:
        if not classification.matches:
            continue
        for match in classification.matches:
            eid = match.entity.entity_id
            if eid not in entity_refs:
                entity_refs[eid] = {"entity": match.entity, "refs": []}
            # å°è¯•ä»å·²åŠ è½½ session è·å–, å¦åˆ™ä»åˆ†ç±»è®°å½•æ„å»º ref
            loaded_session = all_sessions_map.get(classification.session_id)
            if loaded_session:
                ref = classifier.build_session_reference(loaded_session, match)
            else:
                ref = SessionReference(
                    session_id=classification.session_id,
                    file_path=classification.file_path,
                    confidence=match.confidence,
                    start_time=classification.start_time,
                    end_time=classification.end_time,
                    message_count=classification.message_count,
                )
            entity_refs[eid]["refs"].append(ref)

    for eid, data in entity_refs.items():
        entity = data["entity"]
        refs = data["refs"]
        entity_index = index_gen.build_entity_index(entity, refs)
        index_gen.write_entity_index(entity, entity_index)
        print(f"  âœ“ {entity.display_name}: {len(refs)} ä¼šè¯")

    # æ¸…ç†ä¸å†æœ‰ session çš„å®ä½“ç´¢å¼•
    for entity in entities:
        if entity.entity_id not in entity_refs:
            idx_path = settings.project_root / entity.history_dir / "sessions-index.json"
            if idx_path.exists():
                idx_path.unlink()
                print(f"  âœ— {entity.display_name}: å·²ç§»é™¤ç©ºç´¢å¼•")

    # ä¸»ç´¢å¼•å’ŒæŠ¥å‘Š
    index_gen.write_master_index(classifications, history_dir)
    index_gen.write_categorization_report(classifications, entities, history_dir)

    # æœªåˆ†ç±»ä¼šè¯
    uncategorized = [c for c in classifications if c.is_uncategorized]
    if uncategorized:
        uncat_dir = history_dir / "uncategorized"
        uncat_dir.mkdir(parents=True, exist_ok=True)
        uncat_data = {
            "sessions": [c.to_dict() for c in uncategorized],
            "count": len(uncategorized),
        }
        with open(uncat_dir / "sessions.json", "w", encoding="utf-8") as f:
            json.dump(uncat_data, f, ensure_ascii=False, indent=2)

        # ç”Ÿæˆæœªåˆ†ç±»ä¼šè¯çš„å›æ”¾æ–‡ä»¶
        uncat_sessions = []
        for c in uncategorized:
            s = all_sessions_map.get(c.session_id)
            if not s:
                # å¢é‡æ¨¡å¼: ä»æ–‡ä»¶åŠ è½½æœªé‡æ–°æ‰«æçš„ session
                fp = c.file_path
                if fp and Path(fp).exists():
                    s = reader.read_session(fp)
            if s:
                uncat_sessions.append(s)
        if uncat_sessions:
            replay_gen = ReadableReplayGenerator(exclude_thinking=settings.exclude_thinking)
            uncat_files = replay_gen.generate_uncategorized(uncat_sessions, history_dir)
            print(f"  âœ“ Uncategorized: {len(uncategorized)} ä¼šè¯, {len(uncat_files)} å›æ”¾æ–‡ä»¶")
    else:
        # æ¸…ç†ç©ºçš„ uncategorized ç›®å½•
        uncat_replay = history_dir / "uncategorized" / "replay"
        if uncat_replay.exists():
            for old_file in uncat_replay.glob("*.md"):
                old_file.unlink()
        uncat_json = history_dir / "uncategorized" / "sessions.json"
        if uncat_json.exists():
            with open(uncat_json, "w", encoding="utf-8") as f:
                json.dump({"sessions": [], "count": 0}, f, ensure_ascii=False, indent=2)

    # ä¿å­˜æ‰«æçŠ¶æ€ (ç”¨äºå¢é‡æ‰«æ)
    scan_state = {
        "last_scan": datetime.now().isoformat(),
        "file_mtimes": {
            f: os.path.getmtime(f) for f in reader.list_session_files(str(settings.sessions_dir))
        },
    }
    history_dir.mkdir(parents=True, exist_ok=True)
    with open(settings.scan_state_path, "w", encoding="utf-8") as f:
        json.dump(scan_state, f, ensure_ascii=False, indent=2)

    print(f"\nå®Œæˆ! åˆ†ç±» {len(classifications)} ä¸ªä¼šè¯")
    print(f"  å·²åˆ†ç±»: {len(classifications) - len(uncategorized)}")
    print(f"  æœªåˆ†ç±»: {len(uncategorized)}")
    print(f"  ä¸»ç´¢å¼•: {history_dir / 'all-sessions.json'}")
    print(f"  æŠ¥å‘Š: {history_dir / 'categorization-report.md'}")


def _merge_classifications(new_classifications, history_dir):
    """å¢é‡æ‰«ææ—¶, åˆå¹¶æ–°åˆ†ç±»åˆ°å·²æœ‰çš„ all-sessions.json"""
    master_path = history_dir / "all-sessions.json"
    if not master_path.exists():
        return new_classifications

    with open(master_path, "r", encoding="utf-8") as f:
        existing_data = json.load(f)

    # ç”¨ session_id åš key, æ–°åˆ†ç±»è¦†ç›–æ—§åˆ†ç±»
    new_ids = {c.session_id for c in new_classifications}

    # ä»æ—§æ•°æ®é‡å»º SessionClassification å¯¹è±¡ (ä»…ä¿ç•™æœªè¢«é‡æ–°æ‰«æçš„)
    merged = list(new_classifications)
    for s in existing_data.get("sessions", []):
        if s["session_id"] in new_ids:
            continue  # å·²è¢«é‡æ–°æ‰«æ, ä½¿ç”¨æ–°ç»“æœ
        # é‡å»º SessionClassification
        matches = []
        for m in s.get("matches", []):
            eid = m.get("entity_id", "")
            etype_str, _, ename = eid.partition(":")
            try:
                etype = EntityType(etype_str)
            except ValueError:
                continue
            entity = Entity(
                entity_type=etype,
                name=ename,
                display_name=m.get("display_name", ename),
                directory="",
            )
            matches.append(EntityMatch(
                entity=entity,
                confidence=m.get("confidence", 0),
                file_path_score=m.get("file_path_score", 0),
                text_pattern_score=m.get("text_pattern_score", 0),
                keyword_score=m.get("keyword_score", 0),
                matched_messages=m.get("matched_messages", 0),
                total_messages=m.get("total_messages", 0),
                evidence=m.get("evidence", []),
            ))
        classification = SessionClassification(
            session_id=s["session_id"],
            file_path=s.get("file_path", ""),
            matches=matches,
            start_time=s.get("start_time", ""),
            end_time=s.get("end_time", ""),
            message_count=s.get("message_count", 0),
            user_message_count=s.get("user_message_count", 0),
        )
        merged.append(classification)

    return merged


def _load_entity_index(settings, entity_id):
    """æŸ¥æ‰¾å®ä½“å¹¶åŠ è½½å…¶ç´¢å¼•, è¿”å› (entity, entity_index, history_dir) æˆ– None"""
    registry = EntityRegistry(settings.project_root, history_root=settings.history_root)
    entities = registry.discover_all()

    matched_entity = None
    for entity in entities:
        if (
            entity_id in entity.name
            or entity_id in entity.entity_id
            or entity_id.lower() in entity.display_name.lower()
        ):
            matched_entity = entity
            break

    if not matched_entity:
        print(f"æœªæ‰¾åˆ°åŒ¹é…çš„å®ä½“: {entity_id}")
        print("å¯ç”¨å®ä½“:")
        for e in entities:
            print(f"  {e.entity_id} - {e.display_name}")
        return None

    index_path = settings.project_root / matched_entity.history_dir / "sessions-index.json"
    if not index_path.exists():
        print(f"å®ä½“ {matched_entity.display_name} æ²¡æœ‰ç´¢å¼•æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ scanã€‚")
        return None

    with open(index_path, "r", encoding="utf-8") as f:
        index_data = json.load(f)

    from session_history.models.index import EntityIndex, SessionReference

    entity_index = EntityIndex(
        entity_id=index_data["entity_id"],
        entity_type=index_data["entity_type"],
        display_name=index_data["display_name"],
        directory=index_data["directory"],
        last_updated=index_data.get("last_updated", ""),
    )
    for s in index_data.get("sessions", []):
        ref = SessionReference(
            session_id=s["session_id"],
            file_path=s["file_path"],
            confidence=s.get("confidence", 0),
            start_time=s.get("start_time", ""),
            end_time=s.get("end_time", ""),
            message_count=s.get("message_count", 0),
        )
        entity_index.sessions.append(ref)

    history_dir = settings.project_root / matched_entity.history_dir
    return matched_entity, entity_index, history_dir


def cmd_replay(args):
    """ç”Ÿæˆä¼šè¯å›æ”¾"""
    settings = Settings()
    entity_id = args.entity

    # ç‰¹æ®Šå¤„ç†: replay uncategorized
    if entity_id.lower() in ("uncategorized", "uncat"):
        _replay_uncategorized(settings)
        return

    result = _load_entity_index(settings, entity_id)
    if result is None:
        return
    matched_entity, entity_index, history_dir = result

    if args.raw:
        # æ—§æ ¼å¼: å•æ–‡ä»¶ HTML + Markdown
        html_gen = HtmlGenerator(exclude_thinking=settings.exclude_thinking)
        html_path = history_dir / "replay.html"
        html_gen.generate(entity_index, html_path)
        print(f"HTML å›æ”¾: {html_path}")

        md_gen = MarkdownGenerator(exclude_thinking=settings.exclude_thinking)
        md_path = history_dir / "replay.md"
        md_gen.generate(entity_index, md_path)
        print(f"Markdown å›æ”¾: {md_path}")
    else:
        # æ–°æ ¼å¼: æŒ‰ session çš„å¯è¯»å›æ”¾æ–‡ä»¶
        replay_gen = ReadableReplayGenerator(exclude_thinking=settings.exclude_thinking)
        generated_files = replay_gen.generate(entity_index, history_dir)

        # ç”Ÿæˆ replay-index.md
        index_gen = ReplayIndexGenerator()
        index_gen.write_entity_index(entity_index, history_dir, generated_files)

        print(f"Readable replay for {matched_entity.display_name}:")
        for fp in generated_files:
            print(f"  {fp.name}")
        print(f"Index: {history_dir / 'replay-index.md'}")
        print(f"Total: {len(generated_files)} session file(s)")

    print(f"\nå®Œæˆ! ä¸º {matched_entity.display_name} ç”Ÿæˆäº†å›æ”¾æ–‡ä»¶")


def _replay_uncategorized(settings):
    """ç”Ÿæˆæœªåˆ†ç±»ä¼šè¯çš„å›æ”¾"""
    history_dir = settings.history_dir
    uncat_json = history_dir / "uncategorized" / "sessions.json"

    if not uncat_json.exists():
        print("æ²¡æœ‰æœªåˆ†ç±»ä¼šè¯ã€‚è¯·å…ˆè¿è¡Œ scanã€‚")
        return

    with open(uncat_json, "r", encoding="utf-8") as f:
        data = json.load(f)

    reader = JsonlReader(
        exclude_thinking=settings.exclude_thinking,
        exclude_sidechains=settings.exclude_sidechains,
    )

    sessions = []
    for s in data.get("sessions", []):
        fp = s.get("file_path", "")
        if fp and Path(fp).exists():
            sessions.append(reader.read_session(fp))

    if not sessions:
        print("æœªåˆ†ç±»ä¼šè¯çš„ JSONL æ–‡ä»¶ä¸å­˜åœ¨ã€‚")
        return

    replay_gen = ReadableReplayGenerator(exclude_thinking=settings.exclude_thinking)
    generated_files = replay_gen.generate_uncategorized(sessions, history_dir)

    print(f"Readable replay for Uncategorized sessions:")
    for fp in generated_files:
        print(f"  {fp.name}")
    print(f"Output: {history_dir / 'uncategorized' / 'replay'}")
    print(f"Total: {len(generated_files)} session file(s)")
    print(f"\nå®Œæˆ! ä¸º {len(generated_files)} ä¸ªæœªåˆ†ç±»ä¼šè¯ç”Ÿæˆäº†å›æ”¾æ–‡ä»¶")


def cmd_search(args):
    """æœç´¢ä¼šè¯"""
    settings = Settings()
    query = args.query.lower()

    reader = JsonlReader(
        exclude_thinking=settings.exclude_thinking,
        exclude_sidechains=settings.exclude_sidechains,
    )
    session_files = reader.list_session_files(str(settings.sessions_dir))

    print(f"æœç´¢: \"{args.query}\"")
    print(f"æ‰«æ {len(session_files)} ä¸ªä¼šè¯...")
    print("-" * 60)

    total_matches = 0
    for fp in session_files:
        session = reader.read_session(fp)
        matches = []
        for msg in session.messages:
            text = msg.text_content.lower()
            if query in text:
                preview = msg.text_content[:120].replace("\n", " ")
                matches.append({
                    "line": msg.line_number,
                    "type": msg.role or msg.msg_type,
                    "time": msg.timestamp[:19] if msg.timestamp else "",
                    "preview": preview,
                })

        if matches:
            sid = session.session_id[:8]
            time_str = session.start_time[:10] if session.start_time else "N/A"
            print(f"\nğŸ“ {sid}... ({time_str}) - {len(matches)} match(es)")
            for m in matches[:args.limit]:
                print(f"  [{m['type']:9s}] {m['time']} | {m['preview']}")
            if len(matches) > args.limit:
                print(f"  ... and {len(matches) - args.limit} more")
            total_matches += len(matches)

    print(f"\næ€»è®¡: {total_matches} åŒ¹é…")


def cmd_list(args):
    """åˆ—å‡ºä¼šè¯åŠåˆ†ç±»"""
    settings = Settings()
    master_path = settings.history_dir / "all-sessions.json"

    if not master_path.exists():
        print("ä¸»ç´¢å¼•ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ scanã€‚")
        return

    with open(master_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    print("=" * 60)
    print("ä¼šè¯åˆ—è¡¨")
    print("=" * 60)
    print(f"æ€»ä¼šè¯æ•°: {data['total_sessions']}")
    print(f"å·²åˆ†ç±»: {data['categorized']} | æœªåˆ†ç±»: {data['uncategorized']}")
    print("-" * 60)

    for s in data.get("sessions", []):
        sid = s["session_id"][:8]
        time_str = s.get("start_time", "")[:10] or "N/A"
        msg_count = s.get("message_count", 0)
        primary = s.get("primary_entity", "Uncategorized")

        matches_str = ""
        if s.get("matches"):
            match_labels = [
                f"{m['display_name']} ({m['confidence']:.2f})"
                for m in s["matches"][:3]
            ]
            matches_str = " | ".join(match_labels)
        else:
            matches_str = "Uncategorized"

        print(f"\n  {sid}... | {time_str} | {msg_count:3d} msgs")
        print(f"    â†’ {matches_str}")

    # æŒ‰ç±»å‹ç­›é€‰
    if args.type:
        filtered = [
            s for s in data.get("sessions", [])
            if any(m.get("entity_id", "").startswith(args.type) for m in s.get("matches", []))
        ]
        print(f"\n--- Filtered by type '{args.type}': {len(filtered)} sessions ---")


def cmd_stats(args):
    """æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡"""
    settings = Settings()
    master_path = settings.history_dir / "all-sessions.json"

    if not master_path.exists():
        print("ä¸»ç´¢å¼•ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ scanã€‚")
        return

    with open(master_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    print("=" * 60)
    print("åˆ†ç±»ç»Ÿè®¡")
    print("=" * 60)

    total = data["total_sessions"]
    categorized = data["categorized"]
    uncategorized = data["uncategorized"]

    print(f"\næ€»ä¼šè¯æ•°: {total}")
    pct = (categorized / total * 100) if total else 0
    bar = "â–ˆ" * int(pct / 5) + "â–‘" * (20 - int(pct / 5))
    print(f"åˆ†ç±»ç‡:   [{bar}] {pct:.1f}%")
    print(f"å·²åˆ†ç±»: {categorized} | æœªåˆ†ç±»: {uncategorized}")

    # æŒ‰å®ä½“ç±»å‹ç»Ÿè®¡
    type_counts = {}
    entity_counts = {}
    for s in data.get("sessions", []):
        for m in s.get("matches", []):
            etype = m.get("entity_id", "unknown").split(":")[0]
            ename = m.get("display_name", "unknown")
            type_counts[etype] = type_counts.get(etype, 0) + 1
            entity_counts[ename] = entity_counts.get(ename, 0) + 1

    print("\næŒ‰ç±»å‹:")
    for etype, count in sorted(type_counts.items(), key=lambda x: x[1], reverse=True):
        print(f"  {etype:15s}: {count} ä¼šè¯")

    print("\næŒ‰å®ä½“ (Top 10):")
    for ename, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {ename:40s}: {count} ä¼šè¯")

    # å¤šåˆ†ç±»ç»Ÿè®¡
    multi_classified = sum(
        1 for s in data.get("sessions", [])
        if len(s.get("matches", [])) > 1
    )
    print(f"\nå¤šåˆ†ç±»ä¼šè¯: {multi_classified} ({multi_classified/total*100:.1f}% of total)" if total else "")


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description="ä¼šè¯å†å²åˆ†ç±»ç³»ç»Ÿ - åˆ†ç±»å’Œå›æ”¾ Claude Code ä¼šè¯",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # scan
    p_scan = subparsers.add_parser("scan", help="æ‰«æä¼šè¯å¹¶åˆ†ç±»")
    p_scan.add_argument("--sessions-dir", help="ä¼šè¯ JSONL ç›®å½•")
    p_scan.add_argument("--incremental", "-i", action="store_true", help="å¢é‡æ‰«æ")

    # replay
    p_replay = subparsers.add_parser("replay", help="ç”Ÿæˆä¼šè¯å›æ”¾")
    p_replay.add_argument("entity", help="å®ä½“æ ‡è¯† (ç¼–å·ã€åç§°æˆ– entity_id)")
    p_replay.add_argument("--raw", action="store_true", help="ä½¿ç”¨æ—§æ ¼å¼ (å•æ–‡ä»¶ HTML/Markdown)")

    # search
    p_search = subparsers.add_parser("search", help="æœç´¢ä¼šè¯å†…å®¹")
    p_search.add_argument("query", help="æœç´¢å…³é”®è¯")
    p_search.add_argument("--limit", "-n", type=int, default=5, help="æ¯ä¸ªä¼šè¯æ˜¾ç¤ºçš„æœ€å¤§åŒ¹é…æ•°")

    # list
    p_list = subparsers.add_parser("list", help="åˆ—å‡ºä¼šè¯åŠåˆ†ç±»")
    p_list.add_argument("--type", "-t", help="æŒ‰å®ä½“ç±»å‹ç­›é€‰")

    # stats
    p_stats = subparsers.add_parser("stats", help="æ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡")

    args = parser.parse_args()

    if args.command == "scan":
        cmd_scan(args)
    elif args.command == "replay":
        cmd_replay(args)
    elif args.command == "search":
        cmd_search(args)
    elif args.command == "list":
        cmd_list(args)
    elif args.command == "stats":
        cmd_stats(args)
    else:
        parser.print_help()


if __name__ == "__main__":
    main()
