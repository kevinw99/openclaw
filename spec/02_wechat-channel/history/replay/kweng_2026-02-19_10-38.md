# Spec 02: wechat-channel - Session Replay

## Session: 2026-02-19T10:38 ~ 12:25
> Person: kweng | Messages: 142 | Turns: 9

---

### 10:38 - <local-command-caveat>Caveat: The messages below were...

**Prompt:**
> <local-command-caveat>Caveat: The messages below were generated by the user while running local commands. DO NOT respond to these messages or otherwise consider them in your response unless the user explicitly asks you to.</local-command-caveat>

**Result:**
*(no text response â€” tools only)*

---

### 10:38 - <command-name>/login</command-name>

**Prompt:**
> <command-name>/login</command-name>
>             <command-message>login</command-message>
>             <command-args></command-args>

**Result:**
*(no text response â€” tools only)*

---

### 10:38 - <local-command-stdout>Login successful</local-command-stdout...

**Prompt:**
> <local-command-stdout>Login successful</local-command-stdout>

**Result:**
*(no text response â€” tools only)*

---

### 10:38 - 1. can you see spec 01? that is my goal

**Prompt:**
> 1. can you see spec 01? that is my goal
> 2. given that goal, my plan is learn how openclaw works, with the assumption that I can use similar approach to implement the spec 01. So how can I learn enough about openclaw to be able to implement spec 01? What level of details do I need to understand?   can you help me to analyze the openclaw repo and provide a learning plan for me to achieve this goal and provide the right level of abstration to understand the repo and implement spec 01, not too much details but enough to get the job done.
>  

<details>
<summary>Full prompt (541 chars)</summary>

1. can you see spec 01? that is my goal
2. given that goal, my plan is learn how openclaw works, with the assumption that I can use similar approach to implement the spec 01. So how can I learn enough about openclaw to be able to implement spec 01? What level of details do I need to understand?   can you help me to analyze the openclaw repo and provide a learning plan for me to achieve this goal and provide the right level of abstration to understand the repo and implement spec 01, not too much details but enough to get the job done.
 

</details>

**Result:**
Now I have a complete picture. Here's my analysis and learning plan:

---

## Key Finding: Swabble (spec 01) is Already Implemented

Before the learning plan â€” **spec 01 is substantially built already**. Swabble exists as a complete Swift package at `/Swabble/` with ~40 files covering everything in the spec. This changes what you need to learn.

---

## What's Already Done

The full pipeline exists:
```
Mic â†’ AVAudioEngine â†’ SpeechAnalyzer â†’ SpeechTranscriber â†’ WakeWordGate â†’ HookExecutor
```

All CLI commands are implemented: `serve`, `transcribe`, `test-hook`, `mic list/set`, `setup`, `doctor`, `health`, `tail-log`, `status`.

**What's missing (from spec's "open decisions" + "out of scope"):**
- UNIX control socket for `status`/`health` (currently placeholder)
- `launchd` helper (`start/stop/restart` are stubs)
- PII redaction (placeholder boolean, no impl)
- Segment-aware wake gate in CLI (SwabbleKit has it, CLI uses text-only match)

---

## Targeted Learning Plan

You only need to understand **3 layers** of openclaw â€” ignore the rest (TypeScript server, channels, gateway, extensions).

### Layer 1: Swabble's own architecture (2 hours) â€” **must know**

Read these files in order:

| File | Why |
|------|-----|
| `Swabble/Package.swift` | Package targets: SwabbleCore, SwabbleKit, swabble CLI |
| `Swabble/Sources/SwabbleCore/Config/Config.swift` | The Codable config structure â€” everything flows from this |
| `Swabble/Sources/SwabbleCore/Speech/SpeechPipeline.swift` | Core actor: mic â†’ transcription stream |
| `Swabble/Sources/SwabbleKit/WakeWordGate.swift` | Wake detection logic |
| `Swabble/Sources/SwabbleCore/Hooks/HookExecutor.swift` | Shell command dispatch |
| `Swabble/Sources/swabble/Commands/ServeCommand.swift` | The main loop â€” ties everything together |

**Mental model:** `ServeCommand` starts `SpeechPipeline` â†’ iterates `AsyncStream<SpeechSegment>` â†’ checks `WakeWordGate` â†’ fires `HookExecutor`. That's the whole daemon.

### Layer 2: macOS app integration (30 min) â€” **only if you're integrating with the GUI app**

The macOS app at `apps/macos/` imports `SwabbleKit` for its own voice wake UI. The only files that matter:

| File | Why |
|------|-----|
| `apps/macos/Package.swift` lines 1-50 | See how it declares `../../Swabble` as a local dependency |
| `apps/macos/Sources/OpenClaw/VoiceWakeForwarder.swift` | How the GUI app uses SwabbleKit |

You can skip the other ~200 macOS app files.

### Layer 3: openclaw conventions (15 min) â€” **for consistency only**

Read just two things:
- `AGENTS.md` â€” skim the Swift section for naming conventions (`@Observable`, StrictConcurrency, `@MainActor`)
- The Commander CLI pattern: look at `apps/macos/Sources/OpenClawMacCLI/` (3 command files) â€” same pattern Swabble uses

---

## What You Can Ignore Entirely

- `src/` â€” TypeScript/Node.js server (irrelevant to Swabble)
- `extensions/` â€” Telegram, Slack, etc. channel plugins
- `docs/` â€” End-user docs for openclaw server
- `apps/ios/`, `apps/android/` â€” Not related
- `packages/`, `ui/`, `vendor/` â€” Server-side JS infrastructure

---

## Suggested Next Steps

1. **Build Swabble first** â€” run `swift build` in `Swabble/` and see what compiles on macOS 26
2. **Fill the gaps** â€” the UNIX socket for `status`, launchd `start/stop/restart`
3. **Wire segment timing** â€” plumb speech segment timing from `SpeechPipeline` through to the `WakeWordGate` in CLI (the spec calls this out as incomplete)

The openclaw repo itself is mostly a distraction for your goal. The relevant learning is 90% within `Swabble/` itself.

*Tools: Glob (4), Read (4), Task (1) -- /Users/kweng/AI/openclaw/Swabble/Sources/SwabbleCore/Speech/SpeechPipeline.swift, /Users/kweng/AI/openclaw/Swabble/Sources/swabble/Commands/ServeCommand.swift, /Users/kweng/AI/openclaw/Swabble/docs/spec.md, /Users/kweng/AI/openclaw/requests-claw.txt, pattern:**/*.md +3 more*

---

### 10:49 - pwd

**Prompt:**
> pwd

**Result:**
`/Users/kweng/AI/openclaw`

*Tools: Bash (1) -- Print working directory*

---

### 11:56 - I don't get the swabble. It seems swift related, which is...

**Prompt:**
> I don't get the swabble. It seems swift related, which is mobile app development, but I don't see how it is related to openclaw or my spec 01. Can you clarify what swabble is and how it fits into the picture?
> 

**Result:**
My mistake from earlier â€” **Swabble is not spec 01**. Let me correct this clearly.

---

## Clarification

**Spec 01** is at `spec/01_full-context-ai-assistant/` â€” a **Full Context AI Personal Assistant** built on top of OpenClaw. Nothing to do with Swift or mobile apps.

**Swabble** (`Swabble/`) is a completely separate thing â€” a macOS voice daemon (Swift) that's part of the openclaw ecosystem for voice input. It's irrelevant to your goal. Ignore it.

---

## What Spec 01 Actually Is

From `spec/01_full-context-ai-assistant/requirements.md`, the goal is:

> Build a personal AI assistant that deeply knows you, collaborates across all your projects, communicates via any channel (WhatsApp, Telegram, Slack, Discord...), and learns/adapts over time.

**The key architectural decision** (already documented in spec): OpenClaw already provides ~80% of what's needed. The work is mostly **configuration and organization**, not building new systems.

What OpenClaw already gives you (checked off in the spec):
- Multi-channel messaging (WhatsApp, Telegram, Slack, Discord, Signal, iMessage, etc.)
- Memory system (MEMORY.md, daily logs, vector + BM25 search)
- Skills system
- Voice wake / talk mode
- Heartbeat (proactive checks)
- Browser control, shell execution, file access
- Cron jobs, webhooks

What still needs to be done:
- Structured `USER.md` with your comprehensive profile
- Project directory organization (`projects/<name>/`)
- `MEMORY.md` curation strategy
- `HEARTBEAT.md` config for proactive assistance
- Custom skills for project management and learning

---

## Revised Learning Plan

This is now a very different learning path â€” no Swift, no mobile, just understanding OpenClaw as an AI agent platform.

### Layer 1: Core concepts (read these 4 things)

| What | Where | Why |
|------|-------|-----|
| What is openclaw? | `README.md` | High-level: it's a self-hosted AI agent that lives in your messaging apps |
| Workspace files | `docs/concepts/` | AGENTS.md, MEMORY.md, SOUL.md, USER.md â€” these define the agent's identity and context |
| Memory system | `docs/concepts/memory.md` | How short-term, long-term, and vector search work |
| Skills system | `docs/tools/skills.md` | How to add custom capabilities |

### Layer 2: What to configure (the actual implementation)

Based on the spec's "Needs Configuration" section:
1. **USER.md** â€” structured profile (name, preferences, history, projects, work context)
2. **Project directories** â€” `workspace/projects/<name>/` with README, status, notes, decisions
3. **HEARTBEAT.md** â€” proactive check-in behavior
4. **MEMORY.md** â€” curated long-term context about you
5. **Custom skills** â€” memory consolidation, project templates, feedback capture

### Layer 3: What you can skip entirely

- Swift / Swabble â€” irrelevant
- Channel integration code (`extensions/`) â€” already works, just configure
- Server infrastructure (`src/`) â€” don't need to touch it
- `apps/` (iOS/Android/macOS) â€” optional, not core to spec 01

---

Shall I read `docs/concepts/memory.md` and the design.md / tasks.md from spec 01 so we can map out the actual implementation steps?

*Tools: Bash (4), Read (2) -- /Users/kweng/AI/openclaw/spec/01_full-context-ai-assistant/README.md, /Users/kweng/AI/openclaw/spec/01_full-context-ai-assistant/requirements.md -- Find spec and numbered files in repo; List top-level directory; List spec 01 directory*

---

### 12:02 - of cource.

**Prompt:**
> of cource. 
> One  key app is wechat. Does openclaw support it? If not, what does it take to support it? specifically, it should be able to:
> 1. read all wechat messages, understand the relationships, content in every conversation or group
> 2. be able to construct context of what the user is currently at
> 3. can you list more reuqiremnts regarding with wechat, should be similar to apps like Whatsapp except wechat has:
> moments (twitter like sharing)
> 
> 

**Result:**
Now I have everything I need. Here's the full picture:

---

## WeChat Support in OpenClaw: Current State

**No WeChat support.** It's not in the extensions list. This is the full channel list:
```
whatsapp, telegram, signal, discord, slack, imessage, matrix, 
mattermost, msteams, googlechat, line, zalo, feishu, tlon, 
twitch, nostr, nextcloud-talk, bluebubbles
```

---

## Why WeChat is Hard

Unlike WhatsApp (which has [Baileys](https://github.com/WhiskeySockets/Baileys), a stable reverse-engineered web client), WeChat has **no official personal account API**. The landscape:

| Approach | Library | Viability |
|----------|---------|-----------|
| WeChat Web protocol (old) | [itchat](https://pypi.org/project/itchat/) (Python) | **Dead** â€” Tencent shut down web.wechat.com for personal accounts in 2021 |
| RPA/puppet over official app | [Wechaty](https://github.com/wechaty/wechaty) (Node.js) | **Alive but costly** â€” free tier uses a puppet that requires a paid subscription (~$15/mo) for personal accounts |
| Windows PC UI automation | [wxauto](https://github.com/cluic/wxauto) (Python, Windows-only) | **Working but fragile** â€” scrapes the Windows WeChat client UI; breaks on WeChat updates |
| Protocol reverse engineering | Various | **High ban risk** â€” account termination likely |

**Recommended path for openclaw**: [Wechaty](https://wechaty.js.org/docs/wechaty) with its Node.js SDK, since openclaw extensions are TypeScript/Node.js. It has a similar architecture to Baileys (event-driven, wraps the underlying transport). The cost is the WXWork/personal puppet, or running their open-source puppet on your own device.

---

## Requirements: WeChat Channel for OpenClaw

Based on the WhatsApp channel spec, mapped to WeChat's feature set:

### Core Messaging (parity with WhatsApp)
- `WR1` Receive and send direct messages (DMs) with contacts
- `WR2` Receive and send group chat messages (WeChat groups up to 500 members)
- `WR3` @mention detection in groups (trigger bot only when mentioned)
- `WR4` Quote/reply threading (WeChat uses a reply-to model)
- `WR5` Voice message receive (extremely common in China â€” transcribe to text)
- `WR6` Image/video/file receive and send
- `WR7` Sticker/emoji receive (log as `<sticker>` placeholder)
- `WR8` Location message receive
- `WR9` Contact/business card receive
- `WR10` Read receipts (WeChat shows who has read in groups)
- `WR11` Acknowledgment reactions (WeChat supports emoji reactions since 2021)
- `WR12` Typing indicators (where supported)

### WeChat-Specific: Moments (æœ‹å‹åœˆ)
- `WM1` Read Moments feed â€” retrieve recent posts from all contacts
- `WM2` Parse Moment content: text, images, videos, links, location tags
- `WM3` Read likes and comments on a Moment post
- `WM4` Understand relationship signal: who interacts with whom in Moments
- `WM5` Post to Moments (text + images)
- `WM6` Like/comment on a Moment
- `WM7` Detect when someone comments on your Moment (notification)
- `WM8` Moments visibility scope (public / friends only / specific people) â€” read-only awareness

### WeChat-Specific: Channels (è§†é¢‘å·) *(lower priority)*
- `WC1` Read subscribed channel posts (short videos/live streams)
- `WC2` Detect when a followed creator goes live

### WeChat-Specific: Official Accounts (å…¬ä¼—å·) *(for context awareness)*
- `WO1` Read subscribed Official Account articles in feed
- `WO2` Detect new articles from followed accounts

### Identity & Relationship Model
- `WI1` Maintain contact list with display names, remarks (å¤‡æ³¨), and tags
- `WI2` Understand group membership and roles (group owner, admin, members)
- `WI3` Map relationship strength from interaction frequency + Moments engagement
- `WI4` Infer relationship context: family/friend/colleague from tags and group memberships
- `WI5` Track "who is this person" context per contact for LLM consumption

### Context for the AI (spec 01 specific)
- `WX1` Summarize recent DM conversations per contact on demand
- `WX2` Summarize group activity and participants
- `WX3` Build a "social graph" summary from Moments interactions
- `WX4` Detect pending/unanswered messages and surface them
- `WX5` Infer what the user is currently engaged in from recent chat activity
- `WX6` Alert on important messages (from key contacts, keywords)

### Auth & Session
- `WA1` QR code login (scan from WeChat app, Linked Devices style)
- `WA2` Session persistence (re-auth only on expiry)
- `WA3` Multi-account support (personal + work WeChat)
- `WA4` Account ban detection and alerting

---

## Honest Assessment: Complexity vs WhatsApp

| Dimension | WhatsApp (openclaw today) | WeChat |
|-----------|--------------------------|--------|
| Library maturity | Baileys â€” stable, free, open | Wechaty â€” works but paid puppet for personal |
| API risk | Medium (TOS violation) | High (Tencent actively bans bots) |
| Moments support | N/A | Requires separate scraping or puppet access |
| Voice messages | Transcription optional | Critical (must have â€” very common in China) |
| Implementation effort | Already done | ~3-4x harder than WhatsApp |
| Platform | Any OS | Wechaty: any; wxauto: Windows only |

**Bottom line**: WeChat integration is buildable using Wechaty (same TypeScript/Node.js pattern as openclaw's other extensions), but Moments reading is the hardest part â€” most puppet implementations don't expose it cleanly. It would likely require a dedicated Windows or Android bridge to read Moments reliably.

Want me to look at the Wechaty Node.js API to assess how an openclaw extension would be structured?

Sources:
- [Wechaty GitHub](https://github.com/wechaty/wechaty)
- [wxauto GitHub](https://github.com/cluic/wxauto)
- [itchat PyPI](https://pypi.org/project/itchat/)
- [Wechaty docs](https://wechaty.js.org/docs/wechaty)

*Tools: Bash (3), Grep (1), Read (1), WebSearch (3) -- /Users/kweng/AI/openclaw, /Users/kweng/AI/openclaw/docs/channels/whatsapp.md, pattern:wechat|WeChat|we_chat -- List available channel extensions; Look at zalo extension structure as a comparable channel; Look at zalo extension internals*

---

### 12:08 - yes, look at Wechaty and design the openclaw extension...

**Prompt:**
> yes, look at Wechaty and design the openclaw extension structure for WeChat

**Result:**
Now I have enough to design the full extension. Here's the complete architecture:

---

## WeChat Extension Design for OpenClaw

### Puppet Strategy (the critical first decision)

| Puppet | Protocol | Cost | WeChat Web accounts | Moments | Voice | Stability |
|--------|---------|------|--------------------|---------|----|---------|
| `wechat4u` | Web | Free | No (banned post-2017) | No | Partial | Low |
| `padlocal` | iPad protocol | ~$15/mo | Yes | **Yes** | Yes | High |
| `xp` | Windows | Free | Yes | No | Yes | Medium |

**Recommendation: Design for `padlocal` as primary, `wechat4u` as dev/test fallback.** PadLocal is the only puppet that supports Moments and works with modern accounts. The XP puppet requires Windows. Make puppet configurable.

---

### Extension File Structure

```
extensions/wechat/
â”œâ”€â”€ openclaw.plugin.json        # Plugin manifest: id, channels, configSchema
â”œâ”€â”€ package.json                # deps: wechaty, wechaty-puppet-padlocal, wechaty-puppet-wechat4u
â”œâ”€â”€ index.ts                    # Plugin entry: register channel + http handler
â””â”€â”€ src/
    â”œâ”€â”€ config-schema.ts        # Zod schema for ~/.openclaw/openclaw.json wechat section
    â”œâ”€â”€ accounts.ts             # Resolve account config (no API token â€” session file based)
    â”œâ”€â”€ channel.ts              # ChannelPlugin + ChannelDock (capabilities, routing, policy)
    â”œâ”€â”€ runtime.ts              # PluginRuntime singleton (same pattern as Zalo)
    â”œâ”€â”€ bot.ts                  # Wechaty bot lifecycle: create, start, stop, singleton per account
    â”œâ”€â”€ monitor.ts              # Event dispatcher: bot.on('message') â†’ processMessage pipeline
    â”œâ”€â”€ send.ts                 # Outbound: bot.Contact.find() â†’ say(), Room.find() â†’ say()
    â”œâ”€â”€ actions.ts              # WeChat tool actions: react (emoji), forward, recall
    â”œâ”€â”€ onboarding.ts           # QR login flow: scan event â†’ serve QR as PNG or terminal art
    â”œâ”€â”€ probe.ts                # Health check: bot.logonoff() + contact self info
    â”œâ”€â”€ status-issues.ts        # Detect: not logged in, puppet error, disconnected
    â”œâ”€â”€ types.ts                # WeChatMessage, WeChatContact, WeChatRoom types
    â”œâ”€â”€ voice.ts                # Voice msg: toFileBox() â†’ save â†’ transcribe (whisper or OS)
    â”œâ”€â”€ moments.ts              # Moments feed: padlocal only â€” poll + parse â†’ context events
    â””â”€â”€ contact-graph.ts        # Build relationship index from contacts + group memberships
```

---

### Key Differences from Zalo

| Concern | Zalo | WeChat |
|---------|------|--------|
| Auth | Bot API token (server-side) | QR scan â†’ session file (client-side) |
| Connection | Polling loop or webhook | Wechaty event emitter (`bot.on(...)`) |
| Message delivery | REST API call | `contact.say()` / `room.say()` |
| Multi-account | Multiple tokens | Multiple `WechatyBuilder` instances, one per account |
| Session file | None | `~/.openclaw/credentials/wechat/<accountId>/session.json` |

---

### Core Module Breakdown

#### `config-schema.ts`
```typescript
// ~/.openclaw/openclaw.json shape for WeChat
{
  channels: {
    wechat: {
      puppet: "padlocal" | "wechat4u" | "xp",   // default: "padlocal"
      padlocalToken: "puppet_padlocal_xxx",       // required for padlocal
      dmPolicy: "pairing" | "allowlist" | "open" | "disabled",
      allowFrom: ["wechat:wxid_xxx"],             // wxid or remark name
      groupPolicy: "allowlist" | "open" | "disabled",
      requireMention: true,                       // in groups, only respond to @bot
      moments: {
        enabled: false,                           // padlocal only
        pollIntervalSeconds: 300,                 // how often to fetch new moments
        injectAsContext: true,                    // add to session context
      },
      voice: {
        transcribe: true,                         // auto-transcribe voice messages
        provider: "system" | "openai",            // "system" = macOS speech, "openai" = whisper
      },
      ackReaction: { emoji: "ðŸ‘€", direct: true, group: "mentions" },
      accounts: {
        personal: { puppet: "padlocal", padlocalToken: "..." },
        work: { puppet: "xp" },
      }
    }
  }
}
```

#### `bot.ts` â€” Wechaty Lifecycle
```typescript
// Key responsibility: one Wechaty instance per account, managed lifecycle
import { WechatyBuilder } from 'wechaty'

export async function createWechatyBot(account: ResolvedWeChatAccount): Promise<WechatyInstance> {
  const puppet = resolvePuppet(account)   // padlocal | wechat4u | xp
  const bot = WechatyBuilder.build({
    name: `openclaw-${account.accountId}`, // â†’ session file path
    puppet,
    puppetOptions: account.puppet === 'padlocal'
      ? { token: account.padlocalToken }
      : {},
  })
  return bot
}
```

#### `monitor.ts` â€” Event Dispatcher (replaces polling loop)
```typescript
// Wechaty is event-driven, no polling needed
bot
  .on('scan',    (qrcode, status) => handleQrScan(qrcode, status, ctx))
  .on('login',   (user)           => handleLogin(user, ctx))
  .on('logout',  (user, reason)   => handleLogout(user, reason, ctx))
  .on('message', (msg)            => processMessage(msg, account, config, core))
  .on('friendship', (friendship)  => handleFriendRequest(friendship, account, config, core))
  .on('room-join', (room, invitees, inviter) => handleRoomJoin(...))
  .on('error',   (error)          => handleBotError(error, ctx))

await bot.start()
```

#### `monitor.ts` â€” `processMessage` flow
```
msg.self() â†’ skip (own messages)
msg.type() â†’ dispatch:
  Text     â†’ processTextMessage()
  Audio    â†’ voice.transcribe() â†’ processTextMessage()
  Image    â†’ saveMedia() â†’ processMediaMessage()
  Video    â†’ saveMedia() â†’ processMediaMessage()
  Recalled â†’ log recall event
  Contact  â†’ extract contact card â†’ processTextMessage("<contact: Name>")
  Url      â†’ extract URL + title â†’ processTextMessage("<link: title url>")

processTextMessage():
  msg.room() â†’ isGroup = true, chatId = room.id, roomTopic = await room.topic()
  msg.from() â†’ senderId = contact.id, senderName = await contact.name()
  isGroup && requireMention && !await msg.mentionSelf() â†’ skip
  
  â†’ dmPolicy check (same pattern as Zalo)
  â†’ pairing flow if unknown sender
  â†’ core.channel.routing.resolveAgentRoute(...)
  â†’ core.channel.reply.formatAgentEnvelope(...)  [channel: "WeChat", from: senderName, ...]
  â†’ core.channel.reply.dispatchReplyWithBufferedBlockDispatcher(...)
      deliver: â†’ send.ts â†’ contact.say(text) or room.say(text)
```

#### `moments.ts` â€” Moments Feed (padlocal only)
```typescript
// Moments is NOT in Wechaty's standard API â€” padlocal exposes it via a custom API
// The approach: periodic polling using padlocal's getMoments() method

export async function startMomentsPoller(bot: Wechaty, config: WeChatConfig, core: PluginRuntime) {
  // padlocal exposes: bot.puppet.getMoments({ count: 20 })
  // Returns: array of { id, contact, content, images, createTime, likeCount, commentList }

  const poll = async () => {
    const moments = await (bot.puppet as PadLocalPuppet).getMoments({ count: 20 })
    for (const moment of moments) {
      if (isNew(moment)) {
        await injectMomentAsContext(moment, core)
        // Inject into the agent's context as:
        // "[WeChat Moment from <name>: <text> | <image count> images | <like count> likes | <comments>]"
      }
    }
  }
  
  setInterval(poll, config.moments.pollIntervalSeconds * 1000)
}
```

#### `contact-graph.ts` â€” Relationship Index
```typescript
// On login, build a map of all contacts with relationship metadata
// Used by the AI to answer "who is X?" and build social context

export type ContactNode = {
  wxid: string
  displayName: string       // as named in WeChat
  remark: string            // your custom remark (å¤‡æ³¨)
  tags: string[]            // your contact tags
  sharedGroups: string[]    // group names you share
  recentInteraction: Date   // last message timestamp
  momentsLikeCount: number  // how often you like their moments
}
```

#### `send.ts` â€” Outbound Delivery
```typescript
// Different from Zalo: no REST call, use Wechaty bot instance
export async function sendWeChatMessage(to: string, text: string, bot: Wechaty) {
  const isGroup = to.startsWith('@@') || to.includes('@chatroom')
  if (isGroup) {
    const room = await bot.Room.find({ id: to })
    await room?.say(text)
  } else {
    const contact = await bot.Contact.find({ id: to })
    await contact?.say(text)
  }
}
```

---

### Session & Auth Flow

```
openclaw channels login --channel wechat [--account personal]
  â†’ creates WechatyBot with puppet
  â†’ bot.on('scan') â†’ prints QR to terminal (or serves PNG at /wechat/qr)
  â†’ user scans with WeChat app
  â†’ bot.on('login') â†’ session saved to ~/.openclaw/credentials/wechat/<accountId>/session.json
  â†’ session auto-restores on gateway restart (Wechaty profile persistence)
```

---

### Capabilities Declaration (`channel.ts`)

```typescript
capabilities: {
  chatTypes: ["direct", "group"],
  media: true,
  reactions: true,           // emoji reactions (padlocal)
  threads: false,            // WeChat has reply-to but not threads
  polls: false,
  nativeCommands: false,
  blockStreaming: true,
  voice: true,               // NEW: voice message transcription
  moments: true,             // NEW: WeChat-specific, padlocal only
}
```

---

### Implementation Phases

**Phase 1 â€” Core messaging** (parity with other channels)
- `bot.ts`, `monitor.ts`, `send.ts`, `channel.ts`, `config-schema.ts`, `onboarding.ts`
- Text DM and group messages, @mention gating, DM policy / pairing, QR login, session persistence

**Phase 2 â€” Rich media**
- `voice.ts` â€” voice message transcription (critical for China usage)
- Image/video/file receive, media download pipeline

**Phase 3 â€” WeChat-specific**
- `moments.ts` â€” Moments feed polling (padlocal only)
- `contact-graph.ts` â€” relationship indexing
- `actions.ts` â€” emoji reactions, contact card sending

**Phase 4 â€” Spec 01 context**
- Moments â†’ injected as session context for the AI
- Contact graph â†’ available as tool (`wechat_contacts`, `wechat_moments`)
- Unanswered message surfacing via heartbeat

---

### Risk Register

| Risk | Severity | Mitigation |
|------|----------|-----------|
| Tencent bans bot account | High | Use padlocal (lower detection rate than web), don't spam |
| padlocal service goes down | Medium | Design puppet as pluggable; fallback to xp on Windows |
| WeChat updates break puppet | Medium | Pin puppet version; watch wechaty GitHub for patches |
| Moments API not in free puppet | High | Accepted: Moments = padlocal-only, clearly documented |
| Web WeChat blocked (post-2017 accounts) | High | Use padlocal for real accounts, wechat4u for dev/test only |

---

Want me to start writing the actual code for Phase 1 (core messaging), beginning with `config-schema.ts` and `channel.ts`?

Sources:
- [Wechaty puppet providers](https://wechaty.js.org/docs/puppet-providers/)
- [puppet-padlocal GitHub](https://github.com/wechaty/puppet-padlocal)
- [puppet-wechat4u GitHub](https://github.com/wechaty/puppet-wechat4u)
- [Wechaty Message API](https://wechaty.js.org/docs/api/message)

*Tools: Read (5), WebFetch (4), WebSearch (1) -- /Users/kweng/AI/openclaw/extensions/zalo/index.ts, /Users/kweng/AI/openclaw/extensions/zalo/openclaw.plugin.json, /Users/kweng/AI/openclaw/extensions/zalo/src/channel.ts, /Users/kweng/AI/openclaw/extensions/zalo/src/monitor.ts, /Users/kweng/AI/openclaw/extensions/zalo/src/runtime.ts*

---

### 12:14 - can you build a new spec for this first

**Prompt:**
> can you build a new spec for this first

**Result:**
Spec 02 is complete at `spec/02_wechat-channel/`. Here's what was created:

---

**`README.md`** â€” Overview, scope boundaries, key decisions, references to spec 01 as the consumer

**`requirements.md`** â€” 13 functional requirements (FR1â€“FR13) covering:
- Core messaging (DM, groups, media, reactions)
- Voice transcription (FR5)
- Moments feed (FR11)
- Contact graph (FR12)
- Auth, multi-account, health
- Plus non-functional requirements and a risk register

**`design.md`** â€” Full technical architecture:
- System context diagram
- All 13 source files with responsibilities
- Data flows for inbound messages, Moments, and login
- Approach comparison (Wechaty vs wxauto vs protocol hacking)
- Code sketches for each module

**`tasks.md`** â€” 19 tasks across 5 phases with dependency graph:
- Phase 1: Scaffolding (4 tasks)
- Phase 2: Bot lifecycle (4 tasks)
- Phase 3: Message pipeline (5 tasks) â€” critical path
- Phase 4: WeChat features â€” voice, Moments, contact graph (3 tasks)
- Phase 5: Docs & tests (3 tasks)

**`status.md`** â€” Tracking board, decisions log, blockers

Want to start on Phase 1 implementation now?

*Tools: Bash (3), Read (4), Write (5) -- /Users/kweng/AI/openclaw/spec/01_full-context-ai-assistant/design.md, /Users/kweng/AI/openclaw/spec/01_full-context-ai-assistant/tasks.md, /Users/kweng/AI/openclaw/spec/02_wechat-channel/README.md, /Users/kweng/AI/openclaw/spec/02_wechat-channel/design.md, /Users/kweng/AI/openclaw/spec/02_wechat-channel/requirements.md +4 more -- List template directory; Create spec directory; Verify all spec files created*

---
